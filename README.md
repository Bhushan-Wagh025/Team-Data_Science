 # Team-Data_Science

<h1 align="center">Hi ðŸ‘‹, we are "Team-Data_Science"</h1>
<h3 align="center">A team of 20 members for HackBio'2021 Virtual Bioinformatics Internship</h3>
- ðŸ”­ We are currently working on <a href="https://github.com/Bhushan-Wagh025/Team-Data_Science">**Team-Data_Science**</a>

- ðŸ‘¯ HackBio Channel [https://hackbio-internship.github.io/webpage-test/](https://hackbio-internship.github.io/webpage-test/)
- ðŸ“« How to reach us **waghbhushan1123@gmail.com**

<h3 align="left">Language:</h3>
<a href="https://www.python.org" target="_blank"> <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/python/python-original.svg" alt="python" width="40" height="40"/> </a>

### Team Work
**To build a model to accurately detect the presence of Parkinsonâ€™s Disease in an individual.**

## Algorithm used initially: 
<h4 align="center">XGBoost</h3>
XGBoost is a new Machine Learning algorithm designed with speed and performance in mind. XGBoost stands for eXtreme Gradient Boosting and is based on decision trees. In this project, we will import the XGBClassifier from the xgboost library; this is an implementation of the scikit-learn API for XGBoost classification.



## Python libraries used:
scikit-learn, numpy, pandas, matplotlib and xgboost

## Python Built in Packages used:
json warnings re

In this Python machine learning project, using the <a href="https://data-flair.training/blogs/python-libraries/">Python libraries</a> scikit-learn, numpy, pandas, and xgboost, we will build a model using an XGBClassifier. Weâ€™ll load the data, get the features and labels, scale the features, then split the dataset, build an XGBClassifier, and then calculate the accuracy of our model.

Dataset used:
Youâ€™ll need the UCI ML Parkinsons dataset for this; you can <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/">download it here.</a> The dataset has 24 columns and 195 records and is only 39.7 KB.


## Other Algorithms which you have to use to analyse the data after we have used XGBClassifier, and draw comparisons between the results (choose any one) :

<h5 align="center"> 
## Beginners:
logistic regression

## Intermediate/Advanced:
k-nearest neighbors
support vector machines
random forests
bagging algorithms
Naive Bayes

Apart from this we will be generating graphical representation for our data and results:
Histograms
Scatter Plot
Heatmaps
Bar Charts
Box
Etc</h5>














<p align="center">
<p3><a href=""><img align="center" src="https://camo.githubusercontent.com/c58e07fb34a45fd051183258b5860608dd86ac98dd151d0522e0575966082b88/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f6e706d2f73696d706c652d69636f6e7340332e302e312f69636f6e732f747769747465722e737667" alt="tbi_internship" height="20" width="20" data-canonical-src="https://cdn.jsdelivr.net/npm/simple-icons@3.0.1/icons/twitter.svg" style="max-width:100%;"></p3></a>
<p3><a href=""><img align="center" src="https://camo.githubusercontent.com/aecaf87326884e8b0466bb799265a13fee7586246ebda3e066cb7fad82a1fd23/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f6e706d2f73696d706c652d69636f6e7340332e302e312f69636f6e732f696e7374616772616d2e737667" alt="ssiddhaantsharma" height="20" width="20" data-canonical-src="https://cdn.jsdelivr.net/npm/simple-icons@3.0.1/icons/instagram.svg" style="max-width:100%;"></a></p3>
<p3><a href =""><img align="center" src="https://camo.githubusercontent.com/4a20e861b6593d07cef8e8b740e64a866ba7a9916d7e00a9c50c05e93a8096b8/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f6e706d2f73696d706c652d69636f6e7340332e302e312f69636f6e732f796f75747562652e737667" alt="ucrp4skeqrnbax0od3ybyt1w" height="20" width="20" data-canonical-src="https://cdn.jsdelivr.net/npm/simple-icons@3.0.1/icons/youtube.svg" style="max-width:100%;"></a></p3></p>
